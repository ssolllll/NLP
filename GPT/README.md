# What is the GPT?

## Basic  Model : GPT-2
- GPT-2(Generative Pre-trained Transformer 2) : OpenAI에서 개발한 대규모 언어 모델.
- Transformer 아키텍쳐를 사용, 총 1.5억 개의 파라미터

## GPT-2로 처리할 수 있는 과제는?

- Github에 NLP와 관련하여 다음 과제들을 차례대로 수행해보려고 함.

### Task

- Text Generation(텍스트 생성)
- Language Translation(기계 번역)
- Text Summarization(요약)
- Question-Answering(질의응답)
- Chatbot(대화 시스템)
- Text Classification(문서 분류)
- Sentiment Analysis(감성 분석)
- Text Summarization(자동 요약)
- Information Extraction(정보 추출)
- Named Entity Recognition